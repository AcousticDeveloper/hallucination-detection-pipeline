{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"DATASET_NAME = \"TRnlp/MixSub\"\nMODEL_NAME = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\nTRAINED_MODEL_NAME = \"Llama-3.2-1B-Instruct-bnb-4bit-MixSub\"\nTRAINED_MODEL_REPO = f\"AdityaMayukhSom/{TRAINED_MODEL_NAME}\"\nMAX_SEQ_LEN = 2048\nLOAD_IN_4BIT = True\nDTYPE = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:12.835239Z","iopub.execute_input":"2025-03-10T03:29:12.835560Z","iopub.status.idle":"2025-03-10T03:29:12.839821Z","shell.execute_reply.started":"2025-03-10T03:29:12.835537Z","shell.execute_reply":"2025-03-10T03:29:12.838916Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from huggingface_hub import login, create_repo\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token=hf_token)\n# create_repo(TRAINED_MODEL_REPO)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:13.101407Z","iopub.execute_input":"2025-03-10T03:29:13.101704Z","iopub.status.idle":"2025-03-10T03:29:13.254199Z","shell.execute_reply.started":"2025-03-10T03:29:13.101682Z","shell.execute_reply":"2025-03-10T03:29:13.253246Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom pathlib import Path\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:13.306004Z","iopub.execute_input":"2025-03-10T03:29:13.306329Z","iopub.status.idle":"2025-03-10T03:29:13.310702Z","shell.execute_reply.started":"2025-03-10T03:29:13.306306Z","shell.execute_reply":"2025-03-10T03:29:13.309265Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from packaging.version import Version as V\n\ntry:\n    import torch\n    from torch.version import cuda\nexcept Exception as e:\n    raise ImportError(\"Install torch via `pip install torch`\")\n\nv = V(torch.__version__)\nis_ampere = torch.cuda.get_device_capability()[0] >= 8\nxformers = \"xformers==0.0.27\" if v < V(\"2.4.0\") else \"xformers\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif cuda != \"12.1\" and cuda != \"11.8\" and cuda != \"12.4\":\n    raise RuntimeError(f\"CUDA = {cuda} not supported!\")\nif   v <= V('2.1.0'):\n    raise RuntimeError(f\"Torch = {v} too old!\")\nelif v <= V('2.1.1'):\n    x = 'cu{}{}-torch211'\nelif v <= V('2.1.2'):\n    x = 'cu{}{}-torch212'\nelif v  < V('2.3.0'):\n    x = 'cu{}{}-torch220'\nelif v  < V('2.4.0'):\n    x = 'cu{}{}-torch230'\nelif v  < V('2.5.0'):\n    x = 'cu{}{}-torch240'\nelif v  < V('2.6.0'):\n    x = 'cu{}{}-torch250'\nelse:\n    raise RuntimeError(f\"Torch = {v} too new!\")\n\nx = x.format(cuda.replace(\".\", \"\"), \"-ampere\" if is_ampere else \"\")\nprint(f'pip install --upgrade pip && pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:13.510530Z","iopub.execute_input":"2025-03-10T03:29:13.510900Z","iopub.status.idle":"2025-03-10T03:29:13.519811Z","shell.execute_reply.started":"2025-03-10T03:29:13.510871Z","shell.execute_reply":"2025-03-10T03:29:13.518682Z"}},"outputs":[{"name":"stdout","text":"pip install --upgrade pip && pip install \"unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git\"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"%%capture\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install --upgrade pip\n!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton\n!pip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:13.660024Z","iopub.execute_input":"2025-03-10T03:29:13.660343Z","iopub.status.idle":"2025-03-10T03:29:32.147024Z","shell.execute_reply.started":"2025-03-10T03:29:13.660318Z","shell.execute_reply":"2025-03-10T03:29:32.145955Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### **Reference Links For Fine Tunning Llama 3.2 With Unsloth** \n\n1. [Fine-tuning Llama 3.2 Using Unsloth](https://www.kdnuggets.com/fine-tuning-llama-using-unsloth)\n2. [Fine-tuning Llama 3 with Unsloth: A Beginnerâ€™s Guide](https://medium.com/@seekmeai/fine-tuning-llama-3-with-unsloth-a-beginners-guide-d239d48eaf71)","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n\nfast_language_model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = MODEL_NAME,\n    max_seq_length = MAX_SEQ_LEN,\n    dtype = DTYPE,\n    load_in_4bit = LOAD_IN_4BIT\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:32.148406Z","iopub.execute_input":"2025-03-10T03:29:32.148724Z","iopub.status.idle":"2025-03-10T03:29:44.029995Z","shell.execute_reply.started":"2025-03-10T03:29:32.148691Z","shell.execute_reply":"2025-03-10T03:29:44.029246Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.9: Fast Llama patching. Transformers: 4.49.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c692dc51e4d84566adb586681437a83b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3eb1575b92b4e80bca88415d71133f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f92bcf9ac61b4a1a82b161a40fe6c65e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9061163dea054f44972c1215f0b07d51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f12c395d87d4dfdb8eb62f47fe51788"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    fast_language_model, \n    r = 16,\n    target_modules = [\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"gate_proj\",\n    ],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 69,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:44.031442Z","iopub.execute_input":"2025-03-10T03:29:44.031710Z","iopub.status.idle":"2025-03-10T03:29:50.946469Z","shell.execute_reply.started":"2025-03-10T03:29:44.031676Z","shell.execute_reply":"2025-03-10T03:29:50.945564Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.3.9 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nfrom datasets import load_dataset, load_from_disk, Dataset\n\n# https://huggingface.co/docs/datasets/en/loading#hugging-face-hub\ndataset = load_dataset(DATASET_NAME)\n# Changing all the column names to have uniform singular forms\ndataset = dataset.rename_column(\"Highlights\", \"Highlight\")\n\n# Only select 10 for training, 4 for testing, if everything goes well, \n# we can fine tune on a larger dataset, this if for easier handling only\ntrain_dataset = dataset[\"train\"].select(range(10))\neval_dataset = dataset[\"test\"].select(range(4))\n\n# Check train dataset before appending 'Prompt' column\n# train_dataset.to_pandas().head()\n# eval_dataset.to_pandas().head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:50.947939Z","iopub.execute_input":"2025-03-10T03:29:50.948195Z","iopub.status.idle":"2025-03-10T03:29:51.767807Z","shell.execute_reply.started":"2025-03-10T03:29:50.948172Z","shell.execute_reply":"2025-03-10T03:29:51.765962Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"INSTRUCTIONS = \"\"\"\nYou are instructed to generate a scientifically accurate highlight of the provided passage without additional \nsentences such as headings or introductions before or after the generated text as it will be used as summary \nin a custom dataset. The highlight should sound plausible and should not contain incorrect information. Generate \n3-5 concise highlight points from the provided research paper abstract, covering key contributions, methods and \noutcomes. Each point should contain 10 to 15 words only. Return the points in plain text format without bullets.\n\nNo Additional Commentary: Exclude lines like \"Here are 3-5 concise highlight points\".\n\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef format_abstract_highlight_as_prompt(examples: list):  \n    prompts: list[str] = []\n\n    abstracts = examples[\"Abstract\"]\n    highlights = examples['Highlight']\n    \n    for abstract, highlight in zip(abstracts, highlights):\n        row_json = [\n            {\"role\": \"system\", \"content\": INSTRUCTIONS},\n            {\"role\": \"user\", \"content\": abstract},\n            # Must add EOS_TOKEN, otherwise your generation will go on forever!\n            {\"role\": \"assistant\", \"content\": highlight + EOS_TOKEN}\n        ]\n        \n        prompt = tokenizer.apply_chat_template(\n            row_json, \n            tokenize=False, \n            add_generation_prompt=False,\n            return_tensors=\"pt\"\n        )\n\n        prompts.append(prompt)\n        \n    return { \n        \"Prompt\": prompts,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:51.768974Z","iopub.execute_input":"2025-03-10T03:29:51.769256Z","iopub.status.idle":"2025-03-10T03:29:52.760917Z","shell.execute_reply.started":"2025-03-10T03:29:51.769231Z","shell.execute_reply":"2025-03-10T03:29:52.759988Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Append Prompt column on which the model will be trained\n\ntrain_dataset = train_dataset.map(format_abstract_highlight_as_prompt, batched=True)\neval_dataset = eval_dataset.map(format_abstract_highlight_as_prompt, batched=True) \n\n# Check train dataset after adding 'Prompt' column\n# train_dataset.to_pandas().head()\n# eval_dataset.to_pandas().head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:52.761828Z","iopub.execute_input":"2025-03-10T03:29:52.762068Z","iopub.status.idle":"2025-03-10T03:29:53.243009Z","shell.execute_reply.started":"2025-03-10T03:29:52.762048Z","shell.execute_reply":"2025-03-10T03:29:53.241980Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"785a1c1fb4614b6b82b0a01cd261c307"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93eaa3a84d9848898d8e879a7f2fecd4"}},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"train_dataset[0]['Prompt']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:53.243791Z","iopub.execute_input":"2025-03-10T03:29:53.244025Z","iopub.status.idle":"2025-03-10T03:29:53.249440Z","shell.execute_reply.started":"2025-03-10T03:29:53.244005Z","shell.execute_reply":"2025-03-10T03:29:53.248735Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Mar 2025\\n\\nYou are instructed to generate a scientifically accurate highlight of the provided passage without additional \\nsentences such as headings or introductions before or after the generated text as it will be used as summary \\nin a custom dataset. The highlight should sound plausible and should not contain incorrect information. Generate \\n3-5 concise highlight points from the provided research paper abstract, covering key contributions, methods and \\noutcomes. Each point should contain 10 to 15 words only. Return the points in plain text format without bullets.\\n\\nNo Additional Commentary: Exclude lines like \"Here are 3-5 concise highlight points\".<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nRecent field data analyses have shown that lumbar spine fractures occurred more frequently in late model vehicles than the early ones in frontal crashes . Therefore the objective of this study was to investigate risk factors associated with lumbar spine fractures in frontal crashes .<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nRisk factors of lumbar spine fractures in frontal crashes were investigated through parametric simulations. Conflicting effects were found between submarining and lumbar spine fractures. Occupant reclined posture severe crash pulse early pulse peak and vehicle pitch angle could also increase lumbar forces in frontal crashes.<|eot_id|><|eot_id|>'"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom unsloth import is_bfloat16_supported\nfrom unsloth.chat_templates import train_on_responses_only\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:53.251488Z","iopub.execute_input":"2025-03-10T03:29:53.251798Z","iopub.status.idle":"2025-03-10T03:29:53.261431Z","shell.execute_reply.started":"2025-03-10T03:29:53.251746Z","shell.execute_reply":"2025-03-10T03:29:53.260537Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    eval_dataset = eval_dataset,\n    dataset_text_field = \"Prompt\",\n    max_seq_length = MAX_SEQ_LEN,\n    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n    dataset_num_proc = 2,\n    packing = False,\n    args = TrainingArguments(\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        gradient_accumulation_steps=4,\n        eval_strategy=\"steps\",\n        eval_steps=0.2,\n        warmup_steps = 5,\n        # num_train_epochs = 1, # Set this for 1 full training run.\n        max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = TRAINED_MODEL_NAME,\n        report_to = \"none\",\n    )\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:53.262638Z","iopub.execute_input":"2025-03-10T03:29:53.262913Z","iopub.status.idle":"2025-03-10T03:29:57.311047Z","shell.execute_reply.started":"2025-03-10T03:29:53.262887Z","shell.execute_reply":"2025-03-10T03:29:57.309945Z"}},"outputs":[{"name":"stdout","text":"Unsloth: We found double BOS tokens - we shall remove one automatically.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing to [\"Prompt\"] (num_proc=2):   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a933a54cd684310ae4819fbd630c5a3"}},"metadata":{}},{"name":"stdout","text":"Unsloth: We found double BOS tokens - we shall remove one automatically.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing to [\"Prompt\"] (num_proc=2):   0%|          | 0/4 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ea0060397d24b47ad3feb48dedef6bd"}},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"trainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<|start_header_id|>system<|end_header_id|>\",\n    response_part = \"<|start_header_id|>assistant<|end_header_id|>\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:57.312245Z","iopub.execute_input":"2025-03-10T03:29:57.312601Z","iopub.status.idle":"2025-03-10T03:29:58.297426Z","shell.execute_reply.started":"2025-03-10T03:29:57.312564Z","shell.execute_reply":"2025-03-10T03:29:58.296359Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba4ad972c66a402dace36d626ef21d41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/4 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8291bd4856f74ba6b74e86040dfb497c"}},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"# @title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:58.298726Z","iopub.execute_input":"2025-03-10T03:29:58.299114Z","iopub.status.idle":"2025-03-10T03:29:58.305621Z","shell.execute_reply.started":"2025-03-10T03:29:58.299077Z","shell.execute_reply":"2025-03-10T03:29:58.304693Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n4.602 GB of memory reserved.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:58.306404Z","iopub.execute_input":"2025-03-10T03:29:58.306609Z","iopub.status.idle":"2025-03-10T03:32:27.666886Z","shell.execute_reply.started":"2025-03-10T03:29:58.306590Z","shell.execute_reply":"2025-03-10T03:32:27.665969Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 10 | Num Epochs = 60 | Total steps = 60\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 11,272,192/760,547,328 (1.48% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='0' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 0/60 : < :, Epoch 0/60]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# @title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:32:27.667684Z","iopub.execute_input":"2025-03-10T03:32:27.667939Z","iopub.status.idle":"2025-03-10T03:32:27.674728Z","shell.execute_reply.started":"2025-03-10T03:32:27.667916Z","shell.execute_reply":"2025-03-10T03:32:27.673733Z"}},"outputs":[{"name":"stdout","text":"147.048 seconds used for training.\n2.45 minutes used for training.\nPeak reserved memory = 4.602 GB.\nPeak reserved memory for training = 0.0 GB.\nPeak reserved memory % of max memory = 31.219 %.\nPeak reserved memory for training % of max memory = 0.0 %.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"trainer.push_to_hub(\n    commit_message=\"first epoch fine tuning on mixsub\",\n    model_name=TRAINED_MODEL_NAME,\n    # language=\"en\",\n    # finetuned_from=MODEL_NAME,\n    # dataset=DATASET_NAME\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:32:27.675875Z","iopub.execute_input":"2025-03-10T03:32:27.676215Z","iopub.status.idle":"2025-03-10T03:32:31.130374Z","shell.execute_reply.started":"2025-03-10T03:32:27.676184Z","shell.execute_reply":"2025-03-10T03:32:31.129621Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbefbafa047844e88aaf36033bc18c35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da036766842a4fa397511325eb4f40c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"131aedc980f840d9ae183c647c16f40b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/45.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4f8764060bb4a7ab19ad8e4604af54d"}},"metadata":{}},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/AdityaMayukhSom/Llama-3.2-1B-Instruct-bnb-4bit-MixSub/commit/e59d4f4c59cc1b049a5b050ad3d6150f7bb0507b', commit_message='first epoch fine tuning on mixsub', commit_description='', oid='e59d4f4c59cc1b049a5b050ad3d6150f7bb0507b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AdityaMayukhSom/Llama-3.2-1B-Instruct-bnb-4bit-MixSub', endpoint='https://huggingface.co', repo_type='model', repo_id='AdityaMayukhSom/Llama-3.2-1B-Instruct-bnb-4bit-MixSub'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":36}]}