{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1o1YBkRPytG0OHlgSVMpaldLxCB2bDRMo",
      "authorship_tag": "ABX9TyO3nnntOWseWH+M3EZeBxOU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaMayukhSom/hallucination-detection-pipeline/blob/main/DatasetGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "xU8hXVmn6ano"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Smaller Dataset For Easier Processing"
      ],
      "metadata": {
        "id": "k082zW1YNRxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loaded the original dataset from huggingface to generate a sliced smalled version which is easier to process."
      ],
      "metadata": {
        "id": "vK4CEZFcLmsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "k7cd3EY-7nRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9j6T1Dm4bVv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset, load_from_disk, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"TRnlp/MixSub\"\n",
        "ori_dataset_dir = \"/content/drive/MyDrive/Datasets/MixSub/Original\"\n",
        "dup_dataset_dir = \"/content/drive/MyDrive/Datasets/MixSub/Duplicate\"\n",
        "dup_csv_file_name = \"duplicate_mixsub.csv\"\n",
        "num_rows_in_dup = 20"
      ],
      "metadata": {
        "id": "Q3rtW-s5Ki3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we do not mention `split` argument while in `load_dataset` function, three seperate folders will be downloaded for `train`, `test` and `validation`. In that case, the `dataset` object generated can be indexed for the three splits which will then return the dataset which contains the actual rows. Mentioning which split to load during in `load_dataset` will return the actual dataset on which we can index with the attribute names."
      ],
      "metadata": {
        "id": "kNeQUSLBLzLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(ori_dataset_dir):\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    dataset.save_to_disk(ori_dataset_dir)\n",
        "else:\n",
        "    dataset = load_from_disk(ori_dataset_dir)"
      ],
      "metadata": {
        "id": "Tn4pSGQ_5F14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataset['train']\n",
        "test_ds = dataset['test']\n",
        "validation_ds = dataset['validation']"
      ],
      "metadata": {
        "id": "-5A5smBGHUYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selects num rows from the original and generates a small dataset\n",
        "# with first `num_rows_in_dup` entries.\n",
        "small_ds = train_ds.select(range(num_rows_in_dup))\n",
        "\n",
        "# Converts the huggingface Datasets type to Pandas Dataframe type\n",
        "small_pd_df = small_ds.to_pandas()\n",
        "\n",
        "small_pd_df.head()"
      ],
      "metadata": {
        "id": "PVE7WmYaKPMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks whether the path to save the small dataset exists or not, if not, creates the directory to in which it will save the generated file. It writes the small dataset in CSV format, which is different from the original Apache Arrow format."
      ],
      "metadata": {
        "id": "IwKQEc2UMaPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(dup_dataset_dir):\n",
        "    Path(dup_dataset_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "dup_ds_path = os.path.join(dup_dataset_dir, dup_csv_file_name)\n",
        "small_pd_df.to_csv(dup_ds_path, index=True)"
      ],
      "metadata": {
        "id": "qkyCUulqCB-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Genrate Hallucinated Dataset"
      ],
      "metadata": {
        "id": "6LmbiEd7NOzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Unsloth Installation Command Generator"
      ],
      "metadata": {
        "id": "NgKYnPl0UlCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We have to check which Torch version for Xformers (2.3 -> 0.0.27)\n",
        "import torch\n",
        "from packaging.version import Version as V\n",
        "xformers = \"xformers==0.0.27\" if V(torch.__version__) < V(\"2.4.0\") else \"xformers\""
      ],
      "metadata": {
        "id": "XhOT67tLRd-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    raise ImportError('Install torch via `pip install torch`')\n",
        "from packaging.version import Version as V\n",
        "v = V(torch.__version__)\n",
        "cuda = str(torch.version.cuda)\n",
        "is_ampere = torch.cuda.get_device_capability()[0] >= 8\n",
        "if cuda != \"12.1\" and cuda != \"11.8\" and cuda != \"12.4\": raise RuntimeError(f\"CUDA = {cuda} not supported!\")\n",
        "if   v <= V('2.1.0'): raise RuntimeError(f\"Torch = {v} too old!\")\n",
        "elif v <= V('2.1.1'): x = 'cu{}{}-torch211'\n",
        "elif v <= V('2.1.2'): x = 'cu{}{}-torch212'\n",
        "elif v  < V('2.3.0'): x = 'cu{}{}-torch220'\n",
        "elif v  < V('2.4.0'): x = 'cu{}{}-torch230'\n",
        "elif v  < V('2.5.0'): x = 'cu{}{}-torch240'\n",
        "elif v  < V('2.6.0'): x = 'cu{}{}-torch250'\n",
        "else: raise RuntimeError(f\"Torch = {v} too new!\")\n",
        "x = x.format(cuda.replace(\".\", \"\"), \"-ampere\" if is_ampere else \"\")\n",
        "print(f'pip install --upgrade pip && pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"')"
      ],
      "metadata": {
        "id": "gxJP_Xh2R7he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install --upgrade pip\n",
        "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton\n",
        "!pip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "8Zu-BAmYR0q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Unsloth Model"
      ],
      "metadata": {
        "id": "6nfKLrb5UqxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from unsloth import FastLanguageModel"
      ],
      "metadata": {
        "id": "vEPkCOx27y1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "max_seq_length = 2048\n",
        "load_in_4bit = True\n",
        "dtype = None\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "EJmb2OvRUKgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    dtype = dtype\n",
        ")"
      ],
      "metadata": {
        "id": "5SBi673mT9kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup Model For Inference"
      ],
      "metadata": {
        "id": "syZEeiGDZayI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "id": "ON4q00cpY3t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hallucinated Highlight Generator"
      ],
      "metadata": {
        "id": "DKoULzOSU6P5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"\"\"\n",
        "You're instructed to generate scientifically inaccurate or hallucinated highlights of the provided passage\n",
        "only without additional sentences like headings, introductions, or text before or after the generated output\n",
        "as the output will be directly used as highlight in a custom dataset. The highlight should sound plausible\n",
        "but contain incorrect information.Generate 3-5 concise highlight points from the provided research paper abstract,\n",
        "covering key contributions, methods, and outcomes. Each point should contain 10 to 15 words only. Return the\n",
        "points in plain text format without bullets.\n",
        "\n",
        "No Additional Commentary: Exclude lines like \"Here are 3-5 concise highlight points\".\n",
        "\"\"\"\n",
        "\n",
        "def generate_hallucinated_highlights(abstract: str) -> str:\n",
        "    row_json = [\n",
        "        {\"role\": \"system\", \"content\": instruction},\n",
        "        {\"role\": \"user\", \"content\": abstract}\n",
        "    ]\n",
        "\n",
        "    model_prompt = tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=True)\n",
        "    model_inputs = tokenizer(model_prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    model_outputs = model.generate(**model_inputs, max_new_tokens=150, num_return_sequences=1)\n",
        "    decoded_text = tokenizer.batch_decode(model_outputs, skip_special_tokens=False)\n",
        "    hallucination = decoded_text[0].split(\"<|start_header_id|>assistant<|end_header_id|>\")[1].strip().split(\"<|eot_id|>\")[0].strip()\n",
        "    return hallucination"
      ],
      "metadata": {
        "id": "pUQbZ65XJFjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset Loader For Hallucinated Dataset"
      ],
      "metadata": {
        "id": "ITl64NtZQdC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_path = Path(os.path.join(dup_dataset_dir, dup_csv_file_name))\n",
        "custom_df = pd.read_csv(ds_path)\n",
        "abstract_ds = Dataset.from_pandas(custom_df['Abstract'].to_frame())\n",
        "hallucination_dataset = abstract_ds.map(generate_hallucinated_highlights, batched=True,)\n",
        "\n",
        "# Step 5: Save updated dataset\n",
        "# updated_dataset.save_to_disk(\"./mixsub_with_hallucinations\")\n",
        "print(\"Dataset updated and saved successfully!\")"
      ],
      "metadata": {
        "id": "jngWJN7l5UGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L4JPdRXWRz9M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}