{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1o1YBkRPytG0OHlgSVMpaldLxCB2bDRMo",
      "authorship_tag": "ABX9TyOQkqqWeToJ/tMBTkDIAP89",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaMayukhSom/hallucination-detection-pipeline/blob/main/notebooks/DatasetGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Commands\n",
        "\n",
        "These commands needs to be run in every session of Colab. Sets up Google Drive for dataset access, logger format and generates constants used throughout the notebook."
      ],
      "metadata": {
        "id": "ayZrLmdDdRSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Google Drive"
      ],
      "metadata": {
        "id": "XqeBAxOLdfD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "cPfnfipcdeX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup Logger For Logging"
      ],
      "metadata": {
        "id": "QbhiojEyXQwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "log_format = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
        "logging.basicConfig(level=logging.INFO, format=log_format, force=True)\n",
        "logger = logging.getLogger()"
      ],
      "metadata": {
        "id": "Yrrx3Lv_XQMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup Constants"
      ],
      "metadata": {
        "id": "YrU9nnSEdHzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"TRnlp/MixSub\"\n",
        "num_rows_in_dup = 10\n",
        "\n",
        "ori_dataset_dir = \"/content/drive/MyDrive/Datasets/MixSub/Original\"\n",
        "dup_dataset_dir = \"/content/drive/MyDrive/Datasets/MixSub/Duplicate\"\n",
        "hal_dataset_dir = \"/content/drive/MyDrive/Datasets/MixSub/Hallucinated\"\n",
        "\n",
        "dup_csv_file_name = \"duplicate_mixsub.csv\"\n",
        "hal_csv_file_name= \"hallucinated_mixsub.csv\""
      ],
      "metadata": {
        "id": "opZ-nffMdMiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Smaller Dataset For Easier Processing"
      ],
      "metadata": {
        "id": "k082zW1YNRxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loaded the original dataset from huggingface to generate a sliced smalled version which is easier to process."
      ],
      "metadata": {
        "id": "vK4CEZFcLmsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "k7cd3EY-7nRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9j6T1Dm4bVv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset, load_from_disk, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we do not mention `split` argument while in `load_dataset` function, three seperate folders will be downloaded for `train`, `test` and `validation`. In that case, the `dataset` object generated can be indexed for the three splits which will then return the dataset which contains the actual rows. Mentioning which split to load during in `load_dataset` will return the actual dataset on which we can index with the attribute names."
      ],
      "metadata": {
        "id": "kNeQUSLBLzLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(ori_dataset_dir):\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    dataset.save_to_disk(ori_dataset_dir)\n",
        "else:\n",
        "    dataset = load_from_disk(ori_dataset_dir)"
      ],
      "metadata": {
        "id": "Tn4pSGQ_5F14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataset['train']\n",
        "test_ds = dataset['test']\n",
        "validation_ds = dataset['validation']"
      ],
      "metadata": {
        "id": "-5A5smBGHUYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selects num rows from the original and generates a small dataset\n",
        "# with first `num_rows_in_dup` entries.\n",
        "small_ds = train_ds.select(range(num_rows_in_dup))\n",
        "\n",
        "# Converts the huggingface Datasets type to Pandas Dataframe type\n",
        "small_pd_df = small_ds.to_pandas()\n",
        "\n",
        "small_pd_df.head()"
      ],
      "metadata": {
        "id": "PVE7WmYaKPMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks whether the path to save the small dataset exists or not, if not, creates the directory to in which it will save the generated file. It writes the small dataset in CSV format, which is different from the original Apache Arrow format."
      ],
      "metadata": {
        "id": "IwKQEc2UMaPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(dup_dataset_dir):\n",
        "    Path(dup_dataset_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "dup_ds_path = os.path.join(dup_dataset_dir, dup_csv_file_name)\n",
        "small_pd_df.to_csv(dup_ds_path, index=True)"
      ],
      "metadata": {
        "id": "qkyCUulqCB-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Hallucinated Dataset"
      ],
      "metadata": {
        "id": "6LmbiEd7NOzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Unsloth Installation Command Generator"
      ],
      "metadata": {
        "id": "NgKYnPl0UlCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from packaging.version import Version as V\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    from torch.version import cuda\n",
        "except:\n",
        "    raise ImportError('Install torch via `pip install torch`')\n",
        "\n",
        "v = V(torch.__version__)\n",
        "is_ampere = torch.cuda.get_device_capability()[0] >= 8\n",
        "xformers = \"xformers==0.0.27\" if v < V(\"2.4.0\") else \"xformers\"\n",
        "\n",
        "if cuda != \"12.1\" and cuda != \"11.8\" and cuda != \"12.4\":\n",
        "    raise RuntimeError(f\"CUDA = {cuda} not supported!\")\n",
        "if   v <= V('2.1.0'):\n",
        "    raise RuntimeError(f\"Torch = {v} too old!\")\n",
        "elif v <= V('2.1.1'):\n",
        "    x = 'cu{}{}-torch211'\n",
        "elif v <= V('2.1.2'):\n",
        "    x = 'cu{}{}-torch212'\n",
        "elif v  < V('2.3.0'):\n",
        "    x = 'cu{}{}-torch220'\n",
        "elif v  < V('2.4.0'):\n",
        "    x = 'cu{}{}-torch230'\n",
        "elif v  < V('2.5.0'):\n",
        "    x = 'cu{}{}-torch240'\n",
        "elif v  < V('2.6.0'):\n",
        "    x = 'cu{}{}-torch250'\n",
        "else:\n",
        "    raise RuntimeError(f\"Torch = {v} too new!\")\n",
        "\n",
        "x = x.format(cuda.replace(\".\", \"\"), \"-ampere\" if is_ampere else \"\")\n",
        "print(f'pip install --upgrade pip && pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"')"
      ],
      "metadata": {
        "id": "gxJP_Xh2R7he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install --upgrade pip\n",
        "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton\n",
        "!pip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "8Zu-BAmYR0q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Unsloth Model"
      ],
      "metadata": {
        "id": "6nfKLrb5UqxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from typing import Literal\n",
        "from pathlib import Path\n",
        "from unsloth import FastLanguageModel"
      ],
      "metadata": {
        "id": "vEPkCOx27y1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
        "max_seq_length = 2048\n",
        "load_in_4bit = True\n",
        "dtype = None\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "EJmb2OvRUKgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    dtype = dtype\n",
        ")"
      ],
      "metadata": {
        "id": "5SBi673mT9kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup Model For Inference"
      ],
      "metadata": {
        "id": "syZEeiGDZayI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "id": "ON4q00cpY3t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hallucinated Highlight Generator"
      ],
      "metadata": {
        "id": "DKoULzOSU6P5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Dataset Loader For Hallucinated Dataset"
      ],
      "metadata": {
        "id": "ITl64NtZQdC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading only the Abstract section of the dataframe into huggingface dataset\n",
        "ds_path = Path(os.path.join(dup_dataset_dir, dup_csv_file_name))\n",
        "custom_df = pd.read_csv(ds_path)\n",
        "abstract_ds = Dataset.from_pandas(custom_df)\n",
        "# Only take the abstrack column from the dataset\n",
        "# abstract_ds = Dataset.from_pandas(custom_df['Abstract'].to_frame())"
      ],
      "metadata": {
        "id": "G10-SpUUrAku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"\"\"\n",
        "You're instructed to generate scientifically inaccurate or hallucinated highlights of the provided passage\n",
        "only without additional sentences like headings, introductions, or text before or after the generated output\n",
        "as the output will be directly used as highlight in a custom dataset. The highlight should sound plausible\n",
        "but contain incorrect information.Generate 3-5 concise highlight points from the provided research paper abstract,\n",
        "covering key contributions, methods, and outcomes. Each point should contain 10 to 15 words only. Return the\n",
        "points in plain text format without bullets.\n",
        "\n",
        "No Additional Commentary: Exclude lines like \"Here are 3-5 concise highlight points\".\n",
        "\"\"\"\n",
        "\n",
        "def generate_hallucinated_highlights(dataset_row) -> dict[Literal['Hallucination'], str]:\n",
        "    init_identifier = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
        "    term_identifier = \"<|eot_id|>\"\n",
        "\n",
        "    abstract = dataset_row['Abstract']\n",
        "\n",
        "    row_json = [\n",
        "        {\"role\": \"system\", \"content\": instruction},\n",
        "        {\"role\": \"user\", \"content\": abstract}\n",
        "    ]\n",
        "\n",
        "    model_prompt = tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=True)\n",
        "    model_inputs = tokenizer(model_prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    model_outputs = model.generate(**model_inputs, max_new_tokens=150, num_return_sequences=1)\n",
        "    decoded_text = tokenizer.batch_decode(model_outputs, skip_special_tokens=False)\n",
        "\n",
        "    # splits the generated output which contains the prompt followed by init identifier and then the hallucination  text by\n",
        "    # where the assistant started generating\n",
        "    # takes the assistant portion, trims it to remove any whitespace, then splits with the end of token id and fixes up any\n",
        "    # whitespace\n",
        "    hallucination = decoded_text[0].split(start_identifier)[1].split(term_identifier)[0]\n",
        "    hallucination = hallucination.strip().replace(\"\\n\", \" \")\n",
        "\n",
        "    return {\n",
        "        'Hallucination': hallucination,\n",
        "    }"
      ],
      "metadata": {
        "id": "pUQbZ65XJFjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the existing dataset rows into hallucinated highlights, the returned\n",
        "# dictionary from the function passed to map, will automatically be appended\n",
        "# to the dataset on which the map function is being called, and a new dataset\n",
        "# will be returned, so note that mapping does not modify the dataset on which\n",
        "# it is being called on.\n",
        "logger.info(\"hallucinated dataset generation started\")\n",
        "hal_ds = abstract_ds.map(generate_hallucinated_highlights)\n",
        "logger.info(\"hallucinated dataset generation finished\")"
      ],
      "metadata": {
        "id": "jngWJN7l5UGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(hal_dataset_dir):\n",
        "    Path(hal_dataset_dir).mkdir(parents=True, exists_ok=True)\n",
        "\n",
        "hal_ds_path = os.path.join(hal_dataset_dir, hal_csv_file_name)\n",
        "\n",
        "logger.info(\"hallucinated dataset saving started\")\n",
        "hal_ds.to_csv(hal_ds_path, index=False)\n",
        "logger.info(\"hallucinated dataset saving finished\")"
      ],
      "metadata": {
        "id": "-qW_UgB5sBqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CL6C2Ut4w633"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}