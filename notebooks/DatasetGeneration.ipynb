{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaMayukhSom/hallucination-detection-pipeline/blob/main/notebooks/DatasetGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Commands\n",
        "\n",
        "These commands needs to be run in every session of Colab. Sets up Google Drive for dataset access, logger format and generates constants used throughout the notebook."
      ],
      "metadata": {
        "id": "ayZrLmdDdRSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Google Drive"
      ],
      "metadata": {
        "id": "XqeBAxOLdfD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "# enter a huggingface token, which will authenticate this notebook session\n",
        "# on huggingface server, after which we can publish the dataset. An alternate\n",
        "# to `login()` is `notebook_login()`. This will prompt for a token.\n",
        "login(token = HF_TOKEN)"
      ],
      "metadata": {
        "id": "cPfnfipcdeX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Libraries"
      ],
      "metadata": {
        "id": "7GOPmWWOk1aC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install -U loguru\n",
        "%pip install -U unsloth\n",
        "%pip install -U datasets\n",
        "%pip install -U accelerate\n",
        "%pip install -U peft\n",
        "%pip install -U bitsandbytes\n",
        "%pip install -U transformers\n",
        "%pip install -U trl\n",
        "%pip install -U xformers"
      ],
      "metadata": {
        "id": "_kO7fbMwk1LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup Imports And Logger"
      ],
      "metadata": {
        "id": "QbhiojEyXQwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "# log_format = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
        "# logging.basicConfig(level=logging.INFO, format=log_format, force=True)\n",
        "# logger = logging.getLogger()\n",
        "\n",
        "import gc\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import psutil\n",
        "import ctypes\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from loguru import logger\n",
        "from datasets import load_dataset, load_from_disk, Dataset\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported"
      ],
      "metadata": {
        "id": "Yrrx3Lv_XQMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Smaller Dataset For Easier Processing"
      ],
      "metadata": {
        "id": "k082zW1YNRxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loaded the original dataset from huggingface to generate a sliced smalled version which is easier to process."
      ],
      "metadata": {
        "id": "vK4CEZFcLmsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we do not mention `split` argument while in `load_dataset` function, three seperate folders will be downloaded for `train`, `test` and `validation`. In that case, the `dataset` object generated can be indexed for the three splits which will then return the dataset which contains the actual rows. Mentioning which split to load during in `load_dataset` will return the actual dataset on which we can index with the attribute names."
      ],
      "metadata": {
        "id": "kNeQUSLBLzLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_small_dataset(\n",
        "    *,\n",
        "    ori_dataset_dir: str | None = None,\n",
        "    dup_dataset_dir: str | None = None,\n",
        "    dup_csv_file_name: str = \"duplicate_mixsub.csv\",\n",
        "    num_rows_in_dup: int = 2000\n",
        "):\n",
        "    if ori_dataset_dir is not None and os.path.exists(ori_dataset_dir):\n",
        "        dataset = load_from_disk(ori_dataset_dir)\n",
        "    else:\n",
        "        dataset = load_dataset(\"TRnlp/MixSub\")\n",
        "        if ori_dataset_dir is not None:\n",
        "            dataset.save_to_disk(ori_dataset_dir)\n",
        "\n",
        "    train_ds = dataset['train']\n",
        "    test_ds = dataset['test']\n",
        "    validation_ds = dataset['validation']\n",
        "\n",
        "    # Selects num rows from the original and generates a small dataset\n",
        "    # with first `num_rows_in_dup` entries.\n",
        "    small_ds = train_ds.select(range(num_rows_in_dup))\n",
        "\n",
        "    # Converts the huggingface Datasets type to Pandas Dataframe type\n",
        "    small_pd_df = small_ds.to_pandas()\n",
        "\n",
        "    if dup_dataset_dir is not None:\n",
        "        if not os.path.exists(dup_dataset_dir):\n",
        "            Path(dup_dataset_dir).mkdir(parents=True, exist_ok=True)\n",
        "        if dup_csv_file_name is not None:\n",
        "            dup_ds_path = os.path.join(dup_dataset_dir, dup_csv_file_name)\n",
        "            logger.info(\"small csv save started\")\n",
        "            small_pd_df.to_csv(dup_ds_path, index=False)\n",
        "            logger.success(\"small csv save success\")\n",
        "\n",
        "    return small_pd_df"
      ],
      "metadata": {
        "id": "si-a5-ZMb_Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks whether the path to save the small dataset exists or not, if not, creates the directory to in which it will save the generated file. It writes the small dataset in CSV format, which is different from the original Apache Arrow format."
      ],
      "metadata": {
        "id": "IwKQEc2UMaPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Hallucinated Dataset"
      ],
      "metadata": {
        "id": "6LmbiEd7NOzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create and setup unsloth model for inference"
      ],
      "metadata": {
        "id": "6nfKLrb5UqxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inference_model():\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "        dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n",
        "    )\n",
        "\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "5SBi673mT9kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hallucinated Highlight Generator"
      ],
      "metadata": {
        "id": "DKoULzOSU6P5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Dataset Loader For Hallucinated Dataset"
      ],
      "metadata": {
        "id": "ITl64NtZQdC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(\n",
        "    *,\n",
        "    dup_dataset_dir: str | None = None,\n",
        "    dup_csv_file_name: str = \"duplicate_mixsub.csv\"\n",
        "    get_small_dataset = True\n",
        "):\n",
        "    if get_small_dataset and dup_dataset_dir is not None:\n",
        "        # Loading only the Abstract section of the dataframe into huggingface dataset\n",
        "        ds_path = Path(os.path.join(dup_dataset_dir, dup_csv_file_name))\n",
        "        custom_df = pd.read_csv(ds_path, index_col = None)\n",
        "        abstract_ds = Dataset.from_pandas(custom_df)\n",
        "    else:\n",
        "        # Loading the dataset dict from huggingface\n",
        "        abstract_dsd = load_dataset(\"TRnlp/MixSub\")\n",
        "        abstract_ds = abstract_dsd['train']\n",
        "\n",
        "    return abstract_ds"
      ],
      "metadata": {
        "id": "G10-SpUUrAku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_hallucinated_highlights(dataset_row, model, tokenizer):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    device = torch.device(device)\n",
        "\n",
        "    instruction = \"\"\"\n",
        "    You're instructed to generate scientifically inaccurate or hallucinated highlights of the provided passage\n",
        "    only without additional sentences like headings, introductions, or text before or after the generated output\n",
        "    as the output will be directly used as highlight in a custom dataset. The highlight should sound plausible\n",
        "    but contain incorrect information.Generate 3-5 concise highlight points from the provided research paper abstract,\n",
        "    covering key contributions, methods, and outcomes. Each point should contain 10 to 15 words only. Return the\n",
        "    points in plain text format without bullets.\n",
        "\n",
        "    No Additional Commentary: Exclude lines like \"Here are 3-5 concise highlight points\".\n",
        "    \"\"\"\n",
        "\n",
        "    init_identifier = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
        "    term_identifier = \"<|eot_id|>\"\n",
        "\n",
        "    batch_data = [\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": instruction},\n",
        "            {\"role\": \"user\", \"content\": abstract}\n",
        "        ]\n",
        "        for abstract in dataset_row['Abstract']\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        batch_data, add_generation_prompt=True, tokenize = True,\n",
        "        padding = True, truncation = True, return_tensors = 'pt',\n",
        "        return_dict = True\n",
        "    ).to(device)\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True, pad_token_id = tokenizer.eos_token_id)\n",
        "    decoded_texts = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=False)\n",
        "\n",
        "    # the generaed output contains the prompt, init identifier, generated highlight and term identifier\n",
        "    # the following code splits the output with init identifier, takes the second section which contains\n",
        "    # the generated highlight followed by term identifier, now splits the second section based on term\n",
        "    # identifier, takes the first section, which contains only the generated output. Then it strips the\n",
        "    # generated content to get rid of any white spaces from the beginning and the end, and replaces\n",
        "    # newline character with whitespace.\n",
        "\n",
        "    hallucination = [decoded_text.split(init_identifier)[1].split(term_identifier)[0].strip().replace(\"\\n\", \" \") for decoded_text in decoded_texts]\n",
        "\n",
        "    del batch_data\n",
        "    del inputs\n",
        "    del outputs\n",
        "\n",
        "    # row_json = [\n",
        "    #     {\"role\": \"system\", \"content\": instruction},\n",
        "    #     {\"role\": \"user\", \"content\": dataset_row['Abstract']}\n",
        "    # ]\n",
        "\n",
        "    # model_prompt = tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=True)\n",
        "    # model_inputs = tokenizer(model_prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    # model_outputs = model.generate(**model_inputs, max_new_tokens=128, num_return_sequences=1)\n",
        "    # decoded_text = tokenizer.batch_decode(model_outputs[:, model_inputs.input_ids.shape[1]:], skip_special_tokens=False)\n",
        "    # hallucination = decoded_text.split(init_identifier)[1].split(term_identifier)[0].strip().replace(\"\\n\", \" \")\n",
        "\n",
        "    # del model_prompt\n",
        "    # del model_inputs\n",
        "    # del model_outputs\n",
        "    # del decoded_text\n",
        "\n",
        "    del instruction\n",
        "    del init_identifier\n",
        "    del term_identifier\n",
        "\n",
        "    # gc.collect()\n",
        "    # torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        'Hallucination': hallucination,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "pUQbZ65XJFjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_mem_stats():\n",
        "    stats = psutil.virtual_memory()\n",
        "    free_gb = stats.free / 1e9\n",
        "    print(f\"Your runtime has {free_gb:.1f} gigabytes of free RAM\")\n",
        "    used_gb = stats.used / 1e9\n",
        "    print(f\"Your runtime has {used_gb:.1f} gigabytes of used RAM\")\n",
        "    avlb_gb = stats.available / 1e9\n",
        "    print(f\"Your runtime has {avlb_gb:.1f} gigabytes of available RAM\")\n",
        "    ram_gb = stats.total / 1e9\n",
        "    print(f\"Your runtime has {ram_gb:.1f} gigabytes of total RAM\")\n",
        "    print(f\"Your runtime has {stats.percent:.1f}% usage of RAM\")"
      ],
      "metadata": {
        "id": "rJ5WXBnX8nB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_hallucinated_highlight_dataset():\n",
        "    ori_dataset_dir = \"/content/drive/MyDrive/Datasets/MixSub/Original\"\n",
        "    dup_dataset_dir = \"/content/drive/MyDrive/Datasets/MixSub/Duplicate\"\n",
        "\n",
        "    hal_ds_hf_name = \"AdityaMayukhSom/MixSub-With-Hallucinated-Highlights\"\n",
        "    hal_dataset_dir = \"/content/drive/MyDrive/Datasets/MixSub/Hallucinated\"\n",
        "    hal_csv_file_name = \"hallucinated_mixsub.csv\"\n",
        "\n",
        "    # small_pd_df = generate_small_dataset(\n",
        "    #     ori_dataset_dir=ori_dataset_dir,\n",
        "    #     dup_dataset_dir=dup_dataset_dir,\n",
        "    # )\n",
        "    # small_pd_df.head()\n",
        "\n",
        "    abstract_ds = get_dataset(dup_dataset_dir = dup_dataset_dir)\n",
        "    model, tokenizer = get_inference_model()\n",
        "\n",
        "    # Map the existing dataset rows into hallucinated highlights, the returned\n",
        "    # dictionary from the function passed to map, will automatically be appended\n",
        "    # to the dataset on which the map function is being called, and a new dataset\n",
        "    # will be returned, so note that mapping does not modify the dataset on which\n",
        "    # it is being called on.\n",
        "    logger.info(\"hallucinated dataset generation started\")\n",
        "    hal_ds = abstract_ds.map(\n",
        "        lambda row: generate_hallucinated_highlights(row, model, tokenizer),\n",
        "        batched = True,\n",
        "        batch_size = 4,\n",
        "    )\n",
        "    logger.success(\"hallucinated dataset generation finished\")\n",
        "\n",
        "    if not os.path.exists(hal_dataset_dir):\n",
        "        Path(hal_dataset_dir).mkdir(parents=True, exist_ok=True)\n",
        "    hal_ds_path = os.path.join(hal_dataset_dir, hal_csv_file_name)\n",
        "\n",
        "    logger.info(\"started saving hallucinated dataset\")\n",
        "    hal_ds.to_csv(hal_ds_path, index=False)\n",
        "    logger.success(\"hallucinated dataset saved to google drive as csv\")\n",
        "\n",
        "    logger.info(\"started pushing hallucinated dataet to huggingface\")\n",
        "    hal_ds.push_to_hub(hal_ds_hf_name)\n",
        "    logger.success(\"hallucinated dataset saved to huggingface as hf dataset\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print_mem_stats()\n",
        "    libc = ctypes.CDLL(\"libc.so.6\") # clearing cache\n",
        "    libc.malloc_trim(0)\n",
        "    print_mem_stats()\n",
        "    generate_hallucinated_highlight_dataset()"
      ],
      "metadata": {
        "id": "TQ1FpQoOepls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SuFNil-Plb7W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}